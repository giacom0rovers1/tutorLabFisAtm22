---
title: "Precipitation processing with tidyverse - R Notebook"
output: html_notebook
---

**Autore**: Giacomo Roversi

Laboratorio di Fisica dell'Atmosfera A.A. 2022/23

# Esperienza 3

------------------------------------------------------------------------

## Introduzione

In questa esperienza lavoriamo con **dati geo-riferiti** ("geo-referenced" o "spatial"), ovvero campi di variabili geofisiche distribuiti su aree del globo terrestre. Andremo a confrontare i campi di stime di precipitazione derivanti da quattro diverse tipologie di prodotti, ognuno con una sua risoluzione spaziale e temporale.

Sfrutteremo dei tools appositi: il pacchetto `terra` e le funzioni della libreria `rgdal`, per gestire agevolmente le coordinate sferiche e i sistemi di riferimento geografici. Per chi avesse avuto l'occasione di lavorare con dati geo-riferiti in R in precedenza, `terra` è l'evoluzione del pacchetto `raster` e ne mantiene invariati quasi tutti i comandi.

Per approfondire l'analisi di dati spaziali con R consiglio di consultare [Spatial Data Science with R and "terra"](https://rspatial.org/terra/index.html)

## Inizializzazione

Per cominciare inizializziamo l'ambiente di calcolo: cancello eventuali variabili...

```{r}
# Clear the workspace
rm(list=ls())
```

... e carico i pacchetti necessari.

Posso indicare i nomi dei pacchetti in un **vettore di stringhe** e verificare in automatico se sono già presenti nel sistema ed installarli in caso negativo. Per tutti deve sempre avvenire la chiamata `library()` (altre volte abbiamo usato `require()` con la stessa funzione), altrimenti non vengono caricati nell'ambiente di lavoro.

```{r}
# A new method to load packages: 
# If they are not available in the system, they are automatically installed.

packages <- c(
  "tidyverse",
  "lubridate",
  "zoo",
  "terra",
  "rgdal",
  "progress"
)

for(pkg in packages){
  
  if(!require(pkg, character.only=T)){install.packages(pkg)} 
  
  library(pkg, character.only=T)
}

tidyverse_logo()
```

Al solito, imposto la directory principale di lavoro in base alla struttura della mia cartella personale (questa parte è da modificare in base al proprio sistema). Il blocco `if()` mi permette di indicare due indirizzi diversi in base al sistema su cui lancio l'esecuzione (Windows o Linux).

Per questa esperienza consiglio di tenere separati i dati dalla cartella di lavoro principale, quindi vado a specificare due indirizzi: `rootfolder` e `datafolder`.

```{r}
# Define the working directory (in this case, the path is different depending on whether the code is executed on Linux or Windows, but just for my convenience)

if(Sys.info()["sysname"]=="Linux"){
  rootfolder <- "/home/giacom0rovers1/tutorLabFisAtm22/Esp_3/"
  datafolder <- "~/Insync/giacomo.roversi2@studio.unibo.it/OneDrive Biz - Shared/Laboratorio di Fisica dell'Atmosfera/Studenti_Esp3/"
}else{
  rootfolder <- "C:/projects/tutorLabFisAtm22/Esp_3/"
  datafolder <- "C:/Users/grove/OneDrive - Alma Mater Studiorum Università di Bologna/Laboratorio di Fisica dell'Atmosfera/Studenti_Esp3/"
}
```

### Definizione dei nomi dei files e delle cartelle

Preparo gli indirizzi alle sottocartelle di `datafolder`, una per ogni strumento (ATTENZIONE: occupano vari GB!). Come nell'Esperienza 1, `resfolder` conterrà tutti i dati processati e i risultati, `figfolder` le figure.

```{r}
# Define the project directories (relative path)
setwd(rootfolder)

HSAF.folder <- paste0(datafolder, "SAT/")
RG.folder   <- paste0(datafolder, "PLV/")
GPM.folder   <- paste0(datafolder, "GPM/")
ERA5.folder   <- paste0(datafolder, "ERA/")

figfolder  <- "figure/"
resfolder  <- "risultati/"

# Create the project directories, if missing
if(!dir.exists(figfolder)){  dir.create(figfolder) }
if(!dir.exists(resfolder)){  dir.create(resfolder) }

```

*(Nota: Per come è configurato ora, il codice genera queste cartelle dentro la working directory* `rootfolder`*, non dentro `datafolder`. Questo perché magari i dati si trovano su una risorsa esterna (un HDD o un cloud) e perché in ogni caso gli outputs pesano molto poco rispetto ai dati grezzi).*

### Intervallo temporale di interesse

Scelgo gli istanti di inizio e fine dell'intervallo temporale che voglio analizzare. Indico data e ora per mezzo di una stringa testuale nel formato standard "YYYYMMDDhhmm", ovvero il formato default per la funzione `ymd_hm()`, espresso in R dalla stringa `"%Y%m%d%H%M"`. Per tenere il notebook leggero nell'esecuzione seleziono solo il primo giorno disponibile:

```{r}
# Define time interval (between 1 October and 30 November 2016)
sel_START <- "201610010000"
sel_END   <- "201610012300"
```

Tramite la suddetta funzione trasformo le due stringhe in variabili temporali. La funzione `seq()` crea la sequenza temporale di tutte le ore tra inizio (+ 1h) e fine dell'intervallo. La funzione `format()` le riconverte in stringhe testuali, da salvare in quello che sarà il mio array di riferimento della serie temporale, `sel.times` (ovvero *selected time intervals*).

```{r}
# Array of selected time intervals (hours)
sel.times <- format(seq(ymd_hm(sel_START)+hours(1), 
                        ymd_hm(sel_END), 
                        by="1 hour"), "%Y%m%d%H%M")
```

### Fattori di conversione delle unità di misura

Alcuni prodotti sono da convertire in **mm/h** perché sono forniti in altre unità di misura:

```{r}
CorrectionFactor_HSAF = 3600 # kg/m2*s --> mm/h
CorrectionFactor_ERA5 = 1000 # m/h --> mm/h
```

### Definizione della soglia Rain/noRain

Trattando con **stime** di pioggia piuttosto che con **misure** dirette, i prodotti di precipitazione possono contenere valori numerici che, pur non essendo impossibili dal punto di vista fisico, non hanno significatività nel confronto con gli strumenti tradizionali e rischiano di inquinare l'analisi dati. Ad esempio: valori molto prossimi allo zero e più piccoli della minima sensibilità strumentale di un pluviometro.

Imposto un valore minimo di intensità di pioggia "realistica". Userò questo valore sia per filtrare i dati, sia per svolgere le analisi categoriche (occorrenze sopra e sotto soglia):

```{r}
THR <- 0.1 # mm/h
```

## Strutture dei dati spaziali

Gli oggetti di dati georiferiti nel pacchetto `terra` possono avere una **struttura vettoriale** (punti, segmenti, [poligoni]{.underline}) o **matriciale** ([rasters]{.underline}, ovvero campi continui grigliati). Le strutture predisposte per questi tipi di dati sono rispettivamente `SpatVector` e `SpatRaster` (dove "spat" sta chiaramente per "spatial") e si creano con le chiamate `vect()` e `rast()`.

I **rasters** possono avere varie matrici di dati "stratificate" una sull'altra: sono i cosiddetti *layers*. Queste matrici condividono la stessa geometria e geolocalizzazione spaziale. Possono rappresentare diverse variabili allo stesso istante temporale oppure ad esempio vari istanti temporali della stessa variabile. Sono insomma oggetti 3D vincolati su una griglia 2D.

I *layers* (o livelli) di un *raster* possono essere richiamati tramite il loro nome (una stringa) o il loro indice (un numero), inseriti dentro **doppie parentesi quadre**, come per gli elementi di una lista. Agli elementi di un oggetto vettoriale o alle celle di un raster si accede invece con il loro indice dentro singole parentesi quadre, come per gli elementi di un vettore. Vedremo qualche esempio tra poco.

### Confini nazionali

Trattando di dati georiferiti, è fondamentale avere a disposizione un database dei confini nazionali e delle linee di costa. Al seguente link è disponibile un **archivio open source** ([ArcGIS Hub: World Countries (Generalized)](https://hub.arcgis.com/datasets/2b93b06dc0dc4e809d3c8db5cb96ba69_0/)) dei confini di tutti gli stati del mondo (la risorsa è gia scaricata in `datafolder`).

I confini sono forniti sotto forma di *shapefiles*. Si tratta di uno standard per oggetti vettoriali di *sistemi informativi geografici (GIS)*, che consiste non di un singolo files ma in un'insieme di files con lo stesso nome e diverse estensioni, il tutto all'interno di una cartella o di un archivio compresso. In particolare, sono tre le estensioni che troviamo SEMPRE nella cartella:

-   **shp** contiene le informazioni della geometria vera e propria

-   **shx** contiene l'indicizzazione della geometria

-   **dbf** contiene una colonna di attributi per ogni geometria

Tante altre sono opzionali, tra le quali è di nostro interesse:

-   **prj** contiene la descrizione della proiezione geografica utilizzata.

Maggiori informazioni si possono trovare qui: [ESRI Shapefile Technical Description](https://www.esri.com/content/dam/esrisites/sitecore-archive/Files/Pdfs/library/whitepapers/pdfs/shapefile.pdf)

Per i nostri scopi, il pacchetto `terra` importa correttamente lo shapefile semplicemente creando uno `SpatVect` dalla cartella che lo contiene, senza bisogno di richiamare un'estensione in particolare:

```{r}
GlobalBG <- vect(paste0(datafolder,"World_Countries_(Generalized)"))
GlobalBG
```

Lo **shapefile** viene importato come una serie di 251 poligoni (cfr. "*geometry : polygons*"), uno per ogni stato mondiale, ognuno con 7 attributi, che vediamo riportati in un *data frame*. E' importante notare che a questo punto il programma è in possesso di concetti come la geolocalizzazione in latitudine e longitudine di un punto e può esprimere in automatico quella posizione in differenti sistemi di riferimento (Coordinate Reference Systems, CRS). Nel caso di oggetti bidimensionali, come i poligoni degli stati, il software riconosce il **perimetro** dell'oggetto e la differenza tra **dentro e fuori** quel perimetro. Questa comprensione risulterà fondamentale per velocizzare moltissimo alcune operazioni.

Seleziono il poligono dell'**Italia** (riga112):

```{r}
which(GlobalBG$COUNTRY=="Italy")

ItalyBG  <- GlobalBG[112]
plot(ItalyBG, col="grey")

```

Ricapitolando, lo shapefile fornisce due cose fondamentali alla nostra analisi:

-   i **confini** da plottare come riferimento visivo nelle mappe

-   un **dominio comune** a tutti i sensori, con cui mascherare (ovvero ritagliare lungo i bordi) tutti i raster

### Griglia comune "MyGrid"

Per un confronto punto a punto tra le stime dei diversi strumenti, non mi basta ritagliare i prodotti con gli stessi confini, ma devo trasferirli tutti su una **griglia geografica comune**, che chiameremo *MyGrid*.

Imposto manualmente le proprietà della griglia: mi posiziono sull'Italia con risoluzione 0.1 gradi lat/lon.

```{r}
# Select properties of the common grid ("MyGrid"):
MyGrid_res = 0.1
MyGrid_extLON = c(0, 25)
MyGrid_extLAT = c(35, 50)
```

Creo un oggetto *SpatRaster* sull'Italia con le proprietà appena definite (il numero di righe e colonne del raster viene calcolato in automatico come rapporto tra estensione e risoluzione desiderata):

```{r}
MyGrid <- rast(ncol = round(diff(MyGrid_extLON)/MyGrid_res),
               xmin = MyGrid_extLON[1], 
               xmax = MyGrid_extLON[2], 
               nrow = round(diff(MyGrid_extLAT)/MyGrid_res), 
               ymin = MyGrid_extLAT[1], 
               ymax = MyGrid_extLAT[2])

MyGrid

```

Il raster così creato non contiene nessun dato, ma ha tutte le informazioni geometriche e geografiche necessarie per manipolare gli altri raster dei prodotti (che tra poco andremo a generare) in modo che acquistino una forma comune.

## Acquisizione

Inizio ora a importare i dati satellitari nell'ambiente di lavoro. L'obiettivo è, a prescindere dalla forma in cui sono disponibili, di riportarli all'interno di uno *SpatRaster*. La procedura di lettura da files binari o testuali è molto lenta e richiede una certa disponibilità di memoria. Una volta trasformati in rasters, i dati saranno accessibili molto più rapidamente e occuperanno meno spazio.

### A) ERA5

I dati della rianalisi ERA5 sono già forniti in un formato raster stratificato e georiferito (file GRIB). L'importazione sarà quindi piuttosto lineare ed immediata.

```{r}
# Search for files in ERA5.folder with the .grib extension ---------------------
ERA5.files <- dir(ERA5.folder, pattern=".grib")
ERA5.files
```

#### Importare i dati

I dati sono contenuti in un singolo file, che viene importato automaticamente da `rast()`. Il fattore di conversione dell'unità di misura è applicato in un colpo a tutti i dati numerici all'interno del raster.

```{r}
## Import data ------------------------------------------------------------------
ERA5.raster <- rast(paste0(ERA5.folder, ERA5.files)) * CorrectionFactor_ERA5
dim(ERA5.raster)
```

I due messaggi di errore si possono ignorare. Il raster ottenuto ha dimensioni 49 x 53 e ha 1464 layers. Trattandosi di due mesi di dati orari, notiamo come 1464 = 24 x (30+31) sia proprio il numero di ore totali.

#### Nominare i layers

Per accedere ai layers senza errori, voglio nominarli con la stringa temporale dell'ora a cui si riferiscono, formattata nello standard "YYYYMMDDhhmm", ovvero il formato default per la funzione `ymd_hm()`, espresso in R dalla stringa `"%Y%m%d%H%M"`.

Per ottenere velocemente tutti i nomi, leggo dal nome del file GRIB gli istanti di inizio e fine dell'intervallo (si veda il nome del files stampato poco sopra: `"ERA5_2016100100_2016113023.grib"`). Per farlo utilizzo la funzione `substr()` che recupera i caratteri della stringa nelle posizioni rispettivamente da 6 a 15 per l'istante iniziale e da 17 a 26 per l'istante finale.

```{r}
## Assign Date ------------------------------------------------------------------
ERA5.times_START <- substr(ERA5.files,6,15)
ERA5.times_END   <- substr(ERA5.files,17,26)
```

Creo poi l'intera sequenza con la funzione `seq()`, impostando un passo di un'ora.

```{r}
ERA5.times <- format(seq(ymd_h(ERA5.times_START), 
                         ymd_h(ERA5.times_END), 
                         by="1 hour"), "%Y%m%d%H%M")
```

Infine assegno il vettore di stringhe ottenuto ai nomi dei layers.

```{r}
names(ERA5.raster) <- ERA5.times
```

La visualizzazione riassuntiva del raster nella console di R mostra (nell'ordine):

-   la classe dell'oggetto

-   le sue dimensioni (come fosse una matrice 3D)

-   la risoluzione spaziale

-   l'estensione

-   il sistema di coordinate di riferimento (in questo caso ereditato dal GRIB file)

-   l'origine dei dati (memoria RAM o altro)

-   i nomi dei layers che sono correttamente impostati come stringhe "YYYYMMDDhhmm"

-   i valori minimi e massimi del layer, utili per valutare rapidamente il contenuto

```{r}
ERA5.raster
```

#### Stampare le mappe

Il pacchetto terra prevede un **metodo specifico** della funzione base `plot()` per oggetti *SpatRaster* (si veda `?terra::plot`). I valori sono già mostrati in scala colore su una mappa lat/lon. Più oggetti si possono sovrapporre specificando `add = T` come parametro di `plot()` (dal secondo in poi).

```{r}
plot(ERA5.raster[["201610012100"]])
plot(ItalyBG, add =T)
```

Il comando plot di diversi layers in contemporanea crea frames multipli (ma fallisce la stampa dei confini con *add=T*, vedremo in seguito una soluzione):

```{r}
plot(ERA5.raster[[c("201610012100", "201610012200")]])
plot(ItalyBG, add =T)

```

#### Proiettare su una geometria diversa

A questo punto ho ancora un raster a risoluzione 0.25 ° lat/lon. Per trasferire i dati su una griglia con le proprietà di *MyGrid* uso il comando `project()` (in questo caso indico espressamente l'appartenenza al pacchetto `terra` con l'operatore `::` per non rischiare ambiguità). Per risparmiare tempo di calcolo, proietto solo i layers degli istanti che mi interessano, ovvero quelli i cui nomi sono contenuti nelle stringhe temporali di `sel.times`.

```{r}
ERA5.MyGrid <- terra::project(ERA5.raster[[sel.times]], MyGrid)
ERA5.MyGrid
```

Il raster ottenuto NON ha sovrascritto *MyGrid* ma ne ha solo copiato la geometria. Ora il numero di righe e colonne, la risoluzione e l'estensione sono quelli desiderati. Anche il sistema di coordinate di riferimento (**CRS**) è passato a quello standard (**WGS 84**). I layers hanno mantenuto la loro denominazione con la stringa temporale, ma ora ci sono solo quelli contenuti in `sel.times`.

I valori sono stati assegnati alle nuove celle tramite **interpolazione bilineare**. Se andiamo a verificare le impostazioni di `terra::project()` infatti, notiamo come sia il metodo predefinito (`method =` ). Altri metodi di interpolazione disponibili sono `near` (nearest neighbour), `cubic` e `cubicspline`.

Se plotto i due rasters uno accanto all'altro (impostanto `mfrow = c(1,2)` con la funzione `par()`), noto sia l'aumento di risoluzione, sia l'ingrandimento dell'estensione della griglia (anche detta *bounding box*, o *bbox*):

```{r}
par(mfrow = c(1,2))

plot(ERA5.raster[["201610012100"]])
plot(ItalyBG, add =T)

plot(ERA5.MyGrid[[c("201610012100")]])
plot(ItalyBG, add =T)
```

#### Filtri sul raster: clamp e maschere

Se ho bisogno di un filtro rapido per eliminare i valori inferiori a una soglia, posso usare la funzione `clamp()` con l'opzione `values=F`. La utilizzo per portare rapidamente a NA eventuali valori strettamente minori di zero, che non hanno valore fisico:

```{r}
## Filter out negative values -------------------------------------------------
ERA5.MyGrid <- clamp(ERA5.MyGrid, 0, values=F)
```

Posso usare la stessa funzione anche per eliminare velocemente i valori inferiori alla soglia *THR* che ho specificato all'inizio:

```{r}
plot(clamp(ERA5.MyGrid[["201610012100"]], THR, values=F))
plot(ItalyBG, add =T)

```

Così facendo però **perdo l'informazione** sulle celle che avevano stimato valori di intensità validi ma sotto soglia, perché vado a confondere queste celle con quelle che erano già NA per dati non validi o fuori dal dominio. Per questo motivo è meglio creare una **maschera categorica** delle celle sopra (*TRUE*) e sotto (*FALSE*) il valore di soglia *THR*. Le celle non valide rimangono NA anche nella maschera.

Creo la maschera applicando direttamente a tutto il raster la condizione di soglia:

```{r}
ERA5.mask <- ERA5.MyGrid >= THR

plot(ERA5.mask[["201610012100"]])
plot(ItalyBG, add =T)

```

Posso usare la maschera così ottenuta per filtrare i dati, con la funzione `mask()` . L'opzione `maskvalue=FALSE`che i valori che devono essere mascherati sono i *FALSE* della maschera. In questo caso ottengo lo stesso risultato della funzione `clamp()`:

```{r}
plot(mask(ERA5.MyGrid[["201610012100"]], 
          ERA5.mask[["201610012100"]], 
          maskvalue=FALSE))
plot(ItalyBG, add =T)

```

Posso anche usare la maschera direttamente come input per l'analisi categorica (vedi dopo).

Di base utilizzerò `clamp()` per esigenze grafiche (ad esempio per la trasparenza dei valori sotto soglia nelle mappe), per selezionare rapidamente dei sottoinsiemi del campo o per rimuovere i dati non fisici (tipo -9999), mentre userò la maschera per l'analisi statistica delle occorrenze sopra e sotto soglia.

#### Istogrammi

Anche la funzione base `hist()` ha un metodo specifico per oggetti *SpatRaster*: il secondo argomento è il nome del livello selezionato:

```{r}
hist(ERA5.MyGrid, 
     "201610012100", 
     xlab="Hourly average rain rate (mm/h)")
```

Posso utilizzare la funzione `clamp()` appena introdotta per mostrare la distribuzione dei soli valori al di sopra della soglia *THR*:

```{r}
hist(clamp(ERA5.MyGrid, THR, values=F), 
     "201610012100", 
     xlab="Hourly average rain rate (mm/h)")

```

Se non fornisco nessun argomento, gli istogrammi dei vari layers vengono giustapposti come nel caso delle mappe (fino a 16 elementi), diventando illeggibili:

```{r}
hist(clamp(ERA5.MyGrid, THR, values=F), xlab="AvgRR (mm/h)")
```

Per aggregare tutti i livelli assieme, devo accedere direttamente ai valori delle celle. Posso farlo con la funzione `values()` che restituisce la matrice dei dati del raster (n celle x n livelli):

```{r}
dim(values(ERA5.MyGrid))
```

```{r}
hist(values(clamp(ERA5.MyGrid, THR, values=F)), 
     xlab="Hourly average rain rate (mm/h)",
     main="ERA5 Precipitation - 01 Oct. 2016")
```

#### Scatterplots

Anche gli scatterplot sono già predisposti per oggetti SpatRaster. Ovviamente viene garantita la corrispondenza cella per cella. A titolo di esempio mostriamo lo scatterplot tra due layers successivi di ERA5, perché per confrontare prdodotti diversi dobbiamo prima completare il processo di omogeneizzazione delle risoluzioni spaziali e temporali (si veda dopo).

```{r, fig.asp=1, fig.width=4}
plot(clamp(ERA5.MyGrid[["201610012100"]], THR, values=F),
     clamp(ERA5.MyGrid[["201610012200"]], THR, values=F), 
     xlim=c(0,7), ylim=c(0,7), col = "navyblue")
abline(a=0, b=1)
```

Per sfruttare le funzioni più avanzate di `ggplot()` invece è necessario tornare alla struttura dei data frames. Concatenando `values()` e `as.data.frame()` si ottiene un data frame con tante colonne quanti i livelli del raster. La funzione `geom_bin_2D()` effettua il binning dei dati in *x* e *y* e restituisce il conteggio in scala colore, che di seguito è impostata in scala logaritmica per gestire l'accumulo di dati vicino all'origine.

```{r, fig.asp=1, fig.width=4}
clamp(ERA5.MyGrid, THR, values=F) %>%
  
  values() %>%
  as.data.frame() %>%
  
  ggplot(aes(`201610012100`, `201610012200`)) + 
  geom_bin_2d(binwidth = 0.2) +
  coord_fixed() + 
  geom_abline(slope = 1, intercept = 0, alpha = 0.2) +
  scale_fill_binned(type = "viridis", 
                    trans = "log", 
                    breaks = c(2, 5, 10, 25, 50, 100)) +
  theme_bw()
```

### B) H-SAF

I dati H-SAF si trovano sempre dentro dei GRIB, ma la specifica del sistema di coordinate di riferimento non viene interpretato correttamente da `rast()`. Non avendo trovato online nessuna informazione sulla stringa CRS corretta, si sfruttano le coordinate della griglia fornite su un file a parte.

*Nota: i dati H-SAF qui considerati fanno riferimento al prodotto H03A, che si estendeva sul solo emisfero nord sopra l'europa, mentre ora è operativo il prodotto H03B che si estende su tutto il disco di SEVIRI (MSG)*.

```{r}
# Search GRIB files ------------------------------------------------------------
HSAF.files <- dir(HSAF.folder, pattern=".grb")
length(HSAF.files)
```

Il prodotto H-SAF ha dati a 15 minuti. I files presenti nella cartella sono uno per istante temporale. Creo il vettore dei tempi direttamente dai nomi di ogni file, nuovamente utilizzando la funzione base `substr()`.

```{r}
## Assign Date ----------------------------------------------------------------
HSAF.times <- paste0(substr(HSAF.files,5,12),substr(HSAF.files,14,17))
```

#### Filtro sui nomi dei files in lettura

La lettura di così tanti files è lenta. Inoltre leggere files che non verranno poi utilizzati riempie inutilmente la memoria RAM. Andiamo quindi ad effettuare la selezione temporale direttamente sull'array delle stringhe dei nomi dei files (e su quello degli intervalli temporali, che deve restare coerente coi files):

```{r}
## Select only files inside the selected time interval -------------------------
sel_idx <- which(ymd_hm(HSAF.times) >= ymd_hm(sel_START) & 
                   ymd_hm(HSAF.times) <= ymd_hm(sel_END))
HSAF.files <- HSAF.files[sel_idx]
HSAF.times <- HSAF.times[sel_idx]
```

Importo le coordinate dal file esterno di supporto:

```{r}
## Import Coordinates (unstructured grid) -------------------------------------
HSAF.coordinates <- read.table(file(paste0(datafolder,"SAT_coordinates.dat")),
                               sep = " ", 
                               na.string = NA, 
                               as.is = TRUE, 
                               header = FALSE)
```

#### Filtro sul data frame dei dati

I files presenti nella cartella (uno per istante temporale) vengono letti con la funzione `readGDAL` del pacchetto `rgdal`, che riesce ad interpretare autonomamente i GRIB. La lettura avviene all'interno di un ciclo sui vari istanti temporali.

*Nota: Le coordinate sono state ribaltate per garantire la corrispondenza col metodo di indicizzazione di `matrix()`, che apparentemente è opposto a quello con cui sono state scritte le coordinate.*

```{r message=FALSE, warning=FALSE}
## Import data (Precipitation Rate [kg/m2 s]) ------------------------------------
HSAF.df <- HSAF.coordinates[1710000:1,]


for(i in 1:length(HSAF.times)){
  HSAF.df <- cbind(HSAF.df, 
                   matrix(rgdal::readGDAL(paste0(HSAF.folder,HSAF.files[i]), 
                                          silent = TRUE)[[1]], 1710000, 1))
} 
colnames(HSAF.df) <- c("Lon", "Lat", HSAF.times)
dim(HSAF.df)
```

Viene creato un *data frame* con colonne per Lon, Lat e per ogni istante temporale. Possiamo già effettuare un primo filtraggio anche senza avere ancora portato i dati dentro un raster. Filtriamo solo le righe che si riferiscono a coppie lat/lon di nostro interesse (area dell'Italia):

```{r}

HSAF.df_ITA <- HSAF.df %>% filter(
  Lon >= 0,
  Lon <= 30,
  Lat >= 30,
  Lat <= 60
) 
rownames(HSAF.df_ITA) <- 1:nrow(HSAF.df_ITA)
HSAF.df_ITA[121784:121790 ,1:6]
```

#### Rasterizzazione di un data frame

E' arrivato il momento di trasportare i dati rimasti all'interno di un raster, ovver di "rasterizzarli". Non a caso la funzione da utilizzare si chiama `rasterize()`. Questa funzione assegna i valori forniti al parametro `values =`, relativi alle coordinate indicate dalla matrice al primo argomento, ovvero `cbind(HSAF.df_ITA$Lon, HSAF.df_ITA$Lat)`, ad una griglia che eredita le proprietà dal secondo argomento, ovvero *MyGrid*.

Inizialmente viene creato un raster vuoto, *HSAF.raster,* ad immagine di *MyGrid* tramite il comando `rast(MyGrid)`. Gli vengono poi agganciati tanti layers quanti sono gli istanti temporali selezionati, tramite l'applicazoine ricorsiva della funzione `c()` , che combina il raster esistente con il nuovo layer appena creato da `rasterize(),` all'interno del ciclo `for()`.

```{r}
HSAF.raster <- rast(MyGrid)

for(i in 1:length(HSAF.times)){
  HSAF.raster <- c(HSAF.raster, 
                   rasterize(cbind(HSAF.df_ITA$Lon, HSAF.df_ITA$Lat), 
                             MyGrid, 
                             values=as.array(HSAF.df_ITA[[HSAF.times[i]]])
                   ) * CorrectionFactor_HSAF)
}
```

In questo caso non ho bisogno di riproiettare il raster su MyGrid perché ha ereditato le proprietà di MyGrid al momento della sua creazione.

I nomi dei layers vengono poi posti uguali alle stringhe temporali a cui si riferiscono, rispettando la convenzione utilizzata per ERA5.

```{r}
names(HSAF.raster) <- HSAF.times
HSAF.raster
```

Vediamo il plot di un layer con questa volta sovrapposti i confini di tutti gli stati dell'area:

```{r}
plot(HSAF.raster[["201610012057"]])
plot(GlobalBG, add=T)

```

**ATTENZIONE**: il datase di H-SAF a quanto pare non contiene valori minori di 0.1 mm/h: dev'essere già stato filtrato da una soglia prima della nostra acquisizione.

```{r}
min(values(HSAF.raster), na.rm = T)
```

Devo usare una strategia diversa per costruire la maschera: assumo che non ci siano celle con stime non valide o mancanti ma che tutti i NA presenti siano stati valori sotto soglia.

```{r}
HSAF.mask <- !is.na(HSAF.raster)

plot(HSAF.mask[["201610012057"]])
plot(ItalyBG, add =T)
```

### C) GPM-IMERG

Anque in questo caso i dati sono contenuti in matrici che si riferiscono a coordinate fornite esternamente. Si ripetono i passaggi di H-SAF, con qualche piccolo adattamento.

Lettura dei nomi dei files e selezione:

```{r}
## Search IMR files (ASCII) ---------------------------------------------------
GPM.files <- dir(GPM.folder, pattern=".imr")

## Assign Date ----------------------------------------------------------------
GPM.times <- paste0(substr(GPM.files,1,8),substr(GPM.files,11,14))

## Select only files inside the selected time interval -------------------------
sel_idx <- which(ymd_hm(GPM.times) >= ymd_hm(sel_START) & 
                   ymd_hm(GPM.times) <= ymd_hm(sel_END))
GPM.files <- GPM.files[sel_idx]
GPM.times <- GPM.times[sel_idx]
```

Lettura delle matrici di dati (stavolta le salvo temporaneamente in una lista):

```{r}
## Import data (Precipitation Rate [mm/h]) ------------------------------------
GPM.list <- list()

for(i in 1:(length(GPM.files)) ) {
  GPM.list[[i]] <- read.table(file(paste0(GPM.folder,GPM.files[i])), 
                              skip = 181, 
                              nrows = 180,
                              sep = ",", 
                              na.string = NA, 
                              as.is = TRUE, 
                              header = FALSE)
  
  GPM.list[[i]]  <- as.matrix(GPM.list[[i]][-1])
}
names(GPM.list) <- GPM.times
```

Lettura delle coordinate della griglia originale (con qualche manipolazione per garantire la coerenza interna):

```{r}
## Import Coordinates (structured grid) ---------------------------------------
LatGPM <- read.table(file(paste0(GPM.folder,GPM.files[1])), 
                     skip = 361, 
                     nrows = 1,
                     sep = ",", 
                     na.string = NA, 
                     as.is = TRUE, 
                     header = FALSE)
temp   <- LatGPM[[2]]
for(i in 3:length(LatGPM)) temp[i-1] <- LatGPM[[i]]
LatGPM <- temp

LonGPM <- read.table(file(paste0(GPM.folder,GPM.files[1])), 
                     skip = 362, 
                     nrows = 1,
                     sep = ",", 
                     na.string = NA, 
                     as.is = TRUE, 
                     header = FALSE)
temp   <- LonGPM[[2]]
for(i in 3:length(LonGPM)) temp[i-1] <- LonGPM[[i]]
LonGPM <- temp

GPM.coordinates <- 0
for(i in LonGPM) for(j in LatGPM) GPM.coordinates <- c(GPM.coordinates,i,j)
GPM.coordinates <- t(matrix(GPM.coordinates[-1], 2, length(LonGPM)*length(LatGPM)))
```

Rasterizzazione su una geometria ereditata da MyGrid e assegnazione dei nomi dei livelli:

```{r}
## Create SpatRaster object from MyGrid ---------------------------------------
GPM.raster <- rast(MyGrid)

## Rasterize data over SpatRaster object (layr by layer) ----------------------
for(i in 1:length(GPM.times)){
  GPM.raster <- c(GPM.raster, 
                  rasterize(GPM.coordinates, 
                            MyGrid, 
                            values=as.array(t(GPM.list[[GPM.times[i]]]))))
}

## Set layer names ------------------------------------------------------------
names(GPM.raster) <- GPM.times
```

Rimozione di eventuali valori negativi:

```{r}
## Filter out negative values -------------------------------------------------
GPM.raster <- clamp(GPM.raster, 0, values=F)


GPM.raster
```

Plot di esempio:

```{r}
plot(clamp(GPM.raster[["201610012100"]], THR, values=F))
plot(GlobalBG, add =T)

```

### D) RAIN GAUGES

Di nuovo, i dati sono contenuti in matrici che si riferiscono a coordinate fornite esternamente. Si ripetono i passaggi di H-SAF e GPM-IMERG, con poche modifiche per gestire i dati, che in questo caso sono salvati dentro files binari con "parole" di quattro bytes (che è stato un formato standard per molti anni).

```{r}
## Search DAT files -----------------------------------------------------------
RG.files <- dir(RG.folder, pattern=".dat")

## Assign Date ----------------------------------------------------------------
RG.times <- substr(RG.files,1,12)

## Select only files inside the selected time interval -------------------------
sel_idx <- which(ymd_hm(RG.times) >= ymd_hm(sel_START) & 
                   ymd_hm(RG.times) <= ymd_hm(sel_END))
RG.files <- RG.files[sel_idx]
RG.times <- RG.times[sel_idx]

## Import Pixel Coordinates (unstructured grid) -------------------------------
RG.coordinates <- read.table(file(paste0(datafolder, "PLV_coordinates.dat")),
                             sep = "", na.string = NA, as.is = TRUE, header = FALSE,
                             col.names = c("x","y","lat","lon"))

# Transformations for consistent data indexing
RG.coordinates$x <- rev(RG.coordinates$x)
RG.coordinates <- RG.coordinates %>% arrange(x, y)

## Import data (Precipitation Rate [mm/h]) ------------------------------------
nROW <- 444
nCOL <- 912
RG.df <- data.frame(matrix(ncol = 0, nrow = nROW*nCOL))


for(i in 1:(length(RG.files)) ) {
  tt <- RG.times[i]
  RG.df <- cbind(RG.df, readBin(paste0(RG.folder,RG.files[i]), 
                                "numeric", 
                                nROW*nCOL, 
                                size=4))
}
colnames(RG.df) <- RG.times

# Create support data frame with coordinates and id (used for filtering)
latlon <- tibble(id = 1:nrow(RG.coordinates), RG.coordinates[c("lon", "lat")])

## Restrict domain to Mediterranean area (bulk) -------------------------------
latlon <- latlon %>% filter(
  lat >= 35,
  lat <= 50,
  lon >= 0,
  lon <= 25
)

## Create SpatRaster object from MyGrid ---------------------------------------
RG.raster <- rast(MyGrid)

## Rasterize data over SpatRaster object (layr by layer) ----------------------
for(i in 1:length(RG.times)){
  RG.raster <- c(RG.raster, 
                 rasterize(cbind(latlon$lon, latlon$lat), 
                           MyGrid, 
                           values=RG.df[latlon$id,i]))
}

## Set layer names ------------------------------------------------------------
names(RG.raster) <- RG.times

## Filter out negative values -------------------------------------------------
RG.raster <- clamp(RG.raster, 0, values=F)
RG.raster

```

```{r}
plot(RG.raster[["201610012100"]])
plot(ItalyBG, add =T) #, col = "grey85")
# ItalyBGfun(NULL, "red")


```


## Elaborazione 

### Dominio spaziale omogeneo (Italia in questo caso)

mask (per avere base spaziale comune)

```{r}
plot(mask(ERA5.MyGrid[["201610012100"]], ItalyBG))


```

### Funzione media oraria

```{r}
hourlyMean <- function(IN.raster, sel.times){
  #' Aggregates time layers of a SpatRaster object through an hourly mean.
  #' @param IN.raster SpatRaster object with multiple layers. Layer names should be in the "%Y%m%d%H%M" format.
  #' @param sel.times Array of strings of the desired time intervals (hours). This should also be in the "%Y%m%d%H%M" format.
  #' @return Output SpatRaster has the same properties of the input raster and layers corresponding to sel.times
  #' @author Giacomo Roversi
  
  # Read raster time intervals
  IN.times <- names(IN.raster)
  
  # Create empty objects
  OUT.raster <- rast(IN.raster)
  OUT.times  <- character() 
  
  # Progress bar
  bar <- progress_bar$new(format = "  :what [:bar] :current/:total (:percent) eta: :eta | :elapsedfull",
                          total = length(sel.times),
                          clear = FALSE)
  
  # Loop through requested time intervals
  for(tim in sel.times){
    
    # Create time strings for every minute in the selected hour
    interval <- format(seq(ymd_hm(tim) - hours(1) + minutes(1), 
                           ymd_hm(tim), 
                           by="1 min"), "%Y%m%d%H%M")
    
    # Select data rows within the selected hour
    idx <- which(IN.times %in% interval)
    
    if(length(idx) > 0){
      # Add the mean of the selected data to the output raster (new layer)
      OUT.raster <- c(OUT.raster, mean(IN.raster[[idx]]))
      OUT.times  <- c(OUT.times, tim)
    }
    
    bar$tick(tokens = list(what = "hourly mean "))
  }
  
  # Set layer names
  names(OUT.raster) <- OUT.times
  
  # Output
  return(OUT.raster) 
}


```


```{r}
ERA5.final <- mask(ERA5.MyGrid[[sel.times]], ItalyBG)
ERA5.final
```

```{r}
RG.final   <- mask(RG.raster[[sel.times]], ItalyBG)
RG.final

```

### Aggregazione: medie orarie

```{r}
GPM.hourly <- hourlyMean(GPM.raster, sel.times)
GPM.final  <- mask(GPM.hourly, ItalyBG)
GPM.final

HSAF.hourly <- hourlyMean(HSAF.raster, sel.times)
HSAF.final  <- mask(HSAF.hourly, ItalyBG)
HSAF.final

```

```{r}
par(mfrow=c(2,2))
plot(ERA5.final[["201610012100"]], main = "ERA5")
plot(RG.final[["201610012100"]], main = "RainGauges")
plot(HSAF.final[["201610012100"]], main = "H-SAF")
plot(GPM.final[["201610012100"]], main = "GPM-IMERG")

```

**Riassumendo**: mi trovo in input quattro campi georiferiti (rasters) con risoluzioni spaziali e temporali diverse, voglio ottenere in output quattro campi sulla stessa griglia e alla stessa risoluzione (cumulate orarie). Per farlo ho preparato una griglia sull'area dell'Italia e con passo omogeneo e ho predisposto una funzione che calcola le cumulate orarie.

### Salvataggio dei raster final

```{r}
writeRaster(ERA5.final, paste0(resfolder, "ERA5_light.tif"), overwrite=TRUE )
writeRaster(RG.final, paste0(resfolder, "RG_light.tif"), overwrite=TRUE )
writeRaster(GPM.final, paste0(resfolder, "GPM_light.tif"), overwrite=TRUE )
writeRaster(HSAF.final, paste0(resfolder, "HSAF_light.tif"), overwrite=TRUE )

```

Per leggere bisogna usare poi: `rast("nomefile")`

```{r}
ERA5 <- rast(paste0(resfolder, "ERA5_final.tif"))
RG   <- rast(paste0(resfolder, "RG_final.tif"))
GPM <- rast(paste0(resfolder, "GPM_final.tif"))
HSAF <- rast(paste0(resfolder, "HSAF_final.tif"))

```

## Analisi


### Funzione per plot mappe

TODO implementare + colors 

```{r}
plotMap <- function(raster, layerNames){
  colorscale <- c("blue","green","yellow","orange","red")
  
  for(lname in layerNames){
    rastLayer <- raster[[lname]]
    
    plot(
      clamp(rastLayer, 0.2, values=F), 
      main = ymd_hm(lname),
      col  = colorscale,  
      xlab = "Longitude (°E)",
      ylab = "Latitude (°N)",
      panel.first = {
        plot(
          crop(GlobalBG, MyGrid), 
          col="gray90", 
          add=T
        )
        grid()
      }
    )
  }
}


```

```{r}

plotMap(HSAF, "201610012100")

# !! I nomi degli assi rimangono fuori dal plot! Probelma del notebook probabilmente


png(filename = paste0(figfolder, "test.png"),
    width    = 480,
    height   = 322,
    units    = "px")

plotMap(HSAF, "201610012100")

dev.off()

```

TODO Salvataggio immagini 2x2

## PDFs

```{r}
ERA5.data <- as.vector(values(ERA5.final))
hist(ERA5.data)

RG.data <- as.vector(values(RG.final))
hist(RG.data)
```

```{r}
data <- tibble(
  Raingauges = RG.data,
  ERA5 = ERA5.data
) %>% 
  pivot_longer(
    cols = c("Raingauges", "ERA5"),
    names_to = "Product",
    values_to = "Precipitation"
  )

data %>%
  ggplot(aes(Precipitation, color = Product)) + 
  geom_freqpoly(binwidth = 1) + 
  scale_y_log10()

```

## Indicatori statistici

```{r}
statsFun <- function(Var, Ref){
  avgR  = mean(Ref)
  SMP   = length(na.omit(Var))
  COR   = cor(Var, Ref, use = "pairwise.complete.obs")
  ME    = mean(Var - Ref, na.rm = T)
  MAE   = mean(abs(Var - Ref), na.rm = T)
  RMSE  = sqrt(mean((Var - Ref)^2, na.rm = T))
  MAX   = max(Var, na.rm = T)
  nME   = ME/avgR
  nMAE  = MAE/avgR
  CV    = RMSE/avgR
  
  table <- data.frame(SMP, avgR, nME, nMAE, CV, COR, ME, MAE, RMSE, MAX)
  
  return(signif(table, 3))
}

statsFun_continuous <- function(Var, Ref){
  
  # continuous
  CC   <- signif(cor(data$Var, data$Ref, method = "pearson", use = "na.or.complete"), 2)
  mean <- signif(mean(data$Ref, na.rm = T),2)
  me   <- signif(mean(data$Var - data$Ref, na.rm = T), 2)
  ME   <- signif(mean(data$Var - data$Ref, na.rm = T)/mean, 2)
  mae  <- signif(mean(abs(data$Var - data$Ref), na.rm = T), 2)
  MAE  <- signif(mean(abs(data$Var - data$Ref), na.rm = T)/mean, 2)
  rmse <- signif(sqrt(mean((data$Var - data$Ref)^2, na.rm = T)), 2)
  CV   <- signif(rmse/mean, 2)
  SMP  <- nrow(na.omit(data))
  R2   <- signif(CC^2, 2)
  
  
  avgR  = mean(Ref)
  SMP   = length(na.omit(Var))
  COR   = cor(Var, Ref, use = "pairwise.complete.obs")
  ME    = mean(Var - Ref, na.rm = T)
  MAE   = mean(abs(Var - Ref), na.rm = T)
  RMSE  = sqrt(mean((Var - Ref)^2, na.rm = T))
  MAX   = max(Var, na.rm = T)
  nME   = ME/avgR
  nMAE  = MAE/avgR
  CV    = RMSE/avgR
  
  table <- data.frame(SMP, avgR, nME, nMAE, CV, COR, ME, MAE, RMSE, MAX)
  
  return(signif(table, 3))
}

statsFun_categorical <- function(Var, Ref){
  xtab <- xtabs(~ Ref + Var, data = cat, na.action = "na.omit")
  
  
  hits <- xtab['TRUE' ,'TRUE' ]  #[2,2]#
  crej <- xtab['FALSE','FALSE']  #[1,1]#
  miss <- xtab['TRUE' ,'FALSE']  #[2,1]#
  fala <- xtab['FALSE','TRUE' ]  #[1,2]#
  
  FAR  <- fala/(hits + fala)
  BIAS <- (hits + fala)/(hits + miss)
  POD  <-  hits/(hits + miss)       
  TS   <-  hits/(hits + miss + fala)  
  SMP  <- sum(xtab)
  
  ACC  <-  (hits + crej)/(hits + fala + miss + crej)
  POFD <- fala/(fala + crej) 
  
  rand <- (hits+miss)*(hits+fala)/SMP
  ETS  <- (hits - rand)/(hits + miss + fala - rand)
  
  table <- data.frame(SMP, FAR, BIAS, POD, TS, ACC, POFD, ETS)
  
  return(signif(table, 3))
}


```

## Scatterplots

TODO

```{r}
tibble(RG.data,
       ERA5.data) %>%
  ggplot(aes(RG.data, ERA5.data)) +
  geom_bin_2d(binwidth = 0.2) + 
  # scale_fill_continuous(trans= "log") +
  scale_fill_binned(type = "viridis", trans="log", breaks=c(1, 2, 5, 10, 25, 50, 100)) +
  coord_fixed() + 
  theme_bw()
# aggiungere scala colore log

```
